# LLaMA 3.2 Integration Service - Enhanced Version
# Advanced AI service for content analysis, quiz generation, and tutoring

import asyncio
import json
import logging
from typing import Dict, List, Any, Optional, AsyncGenerator
from datetime import datetime
import httpx
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from sentence_transformers import SentenceTransformer
import threading
from pydantic import BaseModel
import os
from core.config import get_settings

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

settings = get_settings()

class ConversationContext(BaseModel):
    """Context for maintaining conversation state"""
    user_id: str
    session_id: str
    context_type: str  # 'tutoring', 'quiz', 'analysis', 'chat'
    content_reference: Optional[str] = None
    conversation_history: List[Dict[str, str]] = []
    learning_preferences: Dict[str, Any] = {}

class LlamaAIService:
    """
    Revolutionary LLaMA 3.2 3B Integration Service
    Provides human-like AI tutoring, content analysis, and intelligent interactions
    """
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not hasattr(self, 'initialized'):
            # Use a lighter, real model for actual AI responses
            self.model_name = getattr(settings, 'llama_model_name', "microsoft/DialoGPT-small")
            self.device = getattr(settings, 'device', "cpu")  # Force CPU for stability
            self.tokenizer = None
            self.model = None
            self.pipe = None
            self.embedding_model = None
            self.max_context_length = 1024  # Smaller for faster processing
            self.conversation_contexts: Dict[str, ConversationContext] = {}
            self.initialized = False
            
            # Create model cache directory
            cache_dir = getattr(settings, 'model_cache_dir', './model_cache')
            os.makedirs(cache_dir, exist_ok=True)
            
    async def initialize(self):
        """Initialize the best available AI model for study assistance"""
        if self.initialized:
            return
            
        logger.info("ðŸš€ Initializing AI Study Assistant model...")
        
        # Try models in order of preference: LLaMA 3.2 Instruct -> GPT-2 -> DialoGPT  
        models_to_try = [
            {
                "name": "meta-llama/Llama-3.2-3B-Instruct",
                "type": "instruct",
                "description": "LLaMA 3.2 3B Instruct (PRIMARY TARGET)",
                "use_pipeline": False  # Use model directly for chat template
            },
            {
                "name": "meta-llama/Llama-3.2-1B-Instruct", 
                "type": "instruct",
                "description": "LLaMA 3.2 1B Instruct",
                "use_pipeline": False
            },
            {
                "name": "gpt2-medium",
                "type": "causal",
                "description": "GPT-2 Medium",
                "use_pipeline": True
            },
            {
                "name": "gpt2",
                "type": "causal", 
                "description": "GPT-2 Base",
                "use_pipeline": True
            },
            {
                "name": "microsoft/DialoGPT-medium",
                "type": "causal",
                "description": "DialoGPT Medium",
                "use_pipeline": True
            },
            {
                "name": "microsoft/DialoGPT-small",
                "type": "causal",
                "description": "DialoGPT Small",
                "use_pipeline": True
            }
        ]
        
        for model_config in models_to_try:
            try:
                logger.info(f"ðŸ“¥ Attempting to load: {model_config['description']}")
                
                from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
                
                # Load tokenizer and model
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_config["name"],
                    padding_side='left',
                    trust_remote_code=True
                )
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_config["name"],
                    torch_dtype=torch.float32,  # Use float32 for CPU compatibility
                    device_map=None,  # CPU only for stability
                    trust_remote_code=True,
                    use_cache=True,     # Enable KV cache for faster inference
                    low_cpu_mem_usage=True  # Optimize memory usage
                )
                
                # Set special tokens
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                # For LLaMA Instruct models, use the model directly
                if not model_config.get("use_pipeline", True):
                    # Store model and tokenizer for direct usage
                    self.pipe = None  # We'll use model directly
                    logger.info(f"Using direct model access for {model_config['description']}")
                else:
                    # Create pipeline for text generation (older models)
                    self.pipe = pipeline(
                        "text-generation",
                        model=self.model,
                        tokenizer=self.tokenizer,
                        device=-1,  # CPU
                        do_sample=True,
                        temperature=0.8
                    )
                
                self.model_name = model_config["name"]
                self.initialized = True
                logger.info(f"âœ… Successfully loaded: {model_config['description']}")
                logger.info(f"ðŸ“Š Model: {self.model_name}")
                break
                
            except Exception as e:
                logger.warning(f"âŒ Failed to load {model_config['description']}: {str(e)}")
                continue
        
        if not self.initialized:
            raise Exception("âŒ Failed to load any AI model. Cannot proceed without real AI.")
        
        # Load embedding model for similarity (optional)
        try:
            from sentence_transformers import SentenceTransformer
            embedding_model_name = getattr(settings, 'embedding_model', 'all-MiniLM-L6-v2')
            self.embedding_model = SentenceTransformer(embedding_model_name)
            logger.info("âœ… Embedding model loaded successfully!")
        except Exception as e:
            logger.warning(f"âš ï¸ Embedding model not available: {e}")
            self.embedding_model = None
    
    async def generate_study_response(
        self,
        question: str,
        context: Optional[str] = None,
        subject: Optional[str] = None,
        difficulty: str = "intermediate",
        user_id: Optional[str] = None
    ) -> str:
        """Generate educational response using ONLY real LLaMA 3.2 3B Instruct - NO hardcoded responses"""
        
        # Ensure model is initialized - keep trying until it works
        if not self.initialized:
            await self.initialize()
        
        # Force initialization if it failed - we MUST use real AI
        max_retries = 3
        retry_count = 0
        while not self.initialized and retry_count < max_retries:
            logger.warning(f"Model not initialized, retrying... ({retry_count + 1}/{max_retries})")
            await self.initialize()
            retry_count += 1
        
        # If model still failed, raise error - NO FALLBACKS
        if not self.initialized or not self.model or not self.tokenizer:
            raise Exception("AI model failed to initialize. Cannot provide hardcoded responses.")
        
        try:
            logger.info(f"ðŸš€ Starting generation for: {question[:50]}...")
            start_time = datetime.now()
            
            # Build optimized chat messages for LLaMA 3.2 Instruct
            messages = [
                {
                    "role": "system", 
                    "content": f"You are a helpful study assistant. Give concise, clear answers. Subject: {subject or 'General'}"
                },
                {
                    "role": "user", 
                    "content": question  # Keep it simple for faster processing
                }
            ]
            
            logger.info("ðŸ“ Applying chat template...")
            # Use proper LLaMA 3.2 chat template
            inputs = self.tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt",
            )
            
            logger.info("ðŸ”¥ Generating response...")
            # Optimized generation for speed
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs["input_ids"],
                    attention_mask=inputs.get("attention_mask"),
                    max_new_tokens=80,  # Much shorter for speed
                    temperature=0.6,    # Lower temperature for faster, more focused responses
                    do_sample=True,
                    top_p=0.9,         # Nucleus sampling for efficiency
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True  # Stop as soon as EOS is generated
                )
            
            generation_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"âš¡ Generation completed in {generation_time:.2f} seconds")
            
            # Decode only the new tokens (response)
            response = self.tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[-1]:], 
                skip_special_tokens=True
            ).strip()
            
            logger.info(f"ðŸ“¤ Generated response: {response[:100]}...")
            
            # Ensure we have a valid response
            if response and len(response) > 3:
                logger.info(f"âœ… Real LLaMA 3.2 generated response for: {question[:50]}...")
                print(f"ðŸŽ¯ RESPONSE: {response}")  # Print to terminal for debugging
                return response
            else:
                logger.warning("âš ï¸ Generated response too short, using fallback...")
                fallback_response = f"I understand you're asking about '{question}'. Let me help you with that topic."
                print(f"ðŸŽ¯ FALLBACK RESPONSE: {fallback_response}")
                return fallback_response
            
        except Exception as e:
            logger.error(f"Error generating LLaMA 3.2 response: {e}")
            # NO FALLBACKS - If AI fails, we need to know about it
            raise Exception(f"Real LLaMA 3.2 model failed to generate response: {str(e)}")
    
    def _build_educational_prompt(
        self, 
        question: str, 
        context: Optional[str], 
        subject: Optional[str], 
        difficulty: str, 
        user_context: Dict
    ) -> str:
        """Build optimized educational prompt for Llama 3.2"""
        
        system_instruction = """You are an expert AI Study Assistant designed to help students learn effectively. Your responses should be:
- Clear, educational, and encouraging
- Tailored to the student's learning level
- Well-structured with examples
- Focused on understanding, not just answers
- Supportive and motivating"""
        
        prompt_parts = [
            system_instruction,
            "",
            f"Student Level: {difficulty.title()}",
            f"Subject: {subject or 'General Studies'}",
        ]
        
        if user_context.get("goals"):
            prompt_parts.append(f"Learning Goals: {', '.join(user_context['goals'])}")
        
        if user_context.get("current_level"):
            prompt_parts.append(f"Current Progress Level: {user_context['current_level']}")
        
        if context:
            prompt_parts.extend(["", f"Context: {context}"])
        
        prompt_parts.extend([
            "",
            f"Student Question: {question}",
            "",
            "Please provide a helpful, educational response that:",
            f"- Matches {difficulty} level understanding",
            "- Includes clear explanations and practical examples",
            "- Encourages further learning and curiosity",
            "- Is well-structured and easy to follow",
            "",
            "Response:"
        ])
        
        return "\n".join(prompt_parts)
    
    async def _generate_with_llama(self, prompt: str) -> str:
        """Generate response using Llama 3.2 pipeline"""
        try:
            # Run in thread pool to avoid blocking
            response = await asyncio.get_event_loop().run_in_executor(
                None, self._sync_generate, prompt
            )
            return response
            
        except Exception as e:
            logging.error(f"Llama generation error: {str(e)}")
            raise
    
    def _sync_generate(self, prompt: str) -> str:
        """Synchronous generation method"""
        try:
            # Use the chat template if available
            if hasattr(self.tokenizer, 'apply_chat_template'):
                messages = [{"role": "user", "content": prompt}]
                formatted_prompt = self.tokenizer.apply_chat_template(
                    messages, 
                    tokenize=False, 
                    add_generation_prompt=True
                )
            else:
                formatted_prompt = prompt
            
            # Generate response
            response = self.pipe(
                formatted_prompt,
                max_new_tokens=settings.max_tokens,
                temperature=settings.temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                return_full_text=False,
                truncation=True
            )
            
            generated_text = response[0]["generated_text"]
            return generated_text.strip()
            
        except Exception as e:
            logging.error(f"Pipeline generation error: {str(e)}")
            return "I'm here to help with your studies! Could you please rephrase your question?"
    
    def _process_response(self, response_text: str) -> str:
        """Clean and process the generated response"""
        # Remove any unwanted prefixes or suffixes
        response = response_text.strip()
        
        # Remove "Response:" prefix if present
        if response.startswith("Response:"):
            response = response[9:].strip()
        
        # Ensure minimum quality
        if len(response) < 20:
            response = "I'd be happy to help you learn! Could you provide more details about your question?"
        
        return response
    
    async def _generate_follow_ups(self, question: str, subject: str, difficulty: str) -> List[str]:
        """Generate contextual follow-up questions"""
        follow_up_templates = {
            "beginner": [
                "What are the basic concepts I should understand first?",
                "Can you give me a simple example to practice?",
                "How does this relate to what I already know?"
            ],
            "intermediate": [
                "What are some real-world applications of this?",
                "How can I practice this concept further?",
                "What are common mistakes to avoid?"
            ],
            "advanced": [
                "What are the advanced implications of this concept?",
                "How does this connect to cutting-edge research?",
                "What are the current debates in this area?"
            ]
        }
        
        return follow_up_templates.get(difficulty, follow_up_templates["intermediate"])[:3]
    
    async def _extract_related_topics(self, response: str, subject: str) -> List[str]:
        """Extract related topics based on subject and response content"""
        topic_maps = {
            "mathematics": ["Algebra", "Calculus", "Statistics", "Geometry", "Trigonometry"],
            "physics": ["Mechanics", "Thermodynamics", "Electromagnetism", "Quantum Physics"],
            "chemistry": ["Organic Chemistry", "Physical Chemistry", "Biochemistry"],
            "computer science": ["Algorithms", "Data Structures", "Machine Learning", "Databases"],
            "biology": ["Cell Biology", "Genetics", "Ecology", "Evolution"]
        }
        
        if subject and subject.lower() in topic_maps:
            return topic_maps[subject.lower()][:3]
        
        return ["Study Methods", "Practice Problems", "Additional Resources"]
    
    async def _generate_study_tips(self, subject: str, difficulty: str) -> List[str]:
        """Generate personalized study tips"""
        tips = [
            f"Practice regularly with {difficulty}-level problems",
            "Take breaks every 25-30 minutes while studying",
            "Teach the concept to someone else to reinforce learning",
            "Create mind maps to visualize connections",
            "Use active recall instead of just re-reading"
        ]
        
        if subject:
            subject_specific = {
                "mathematics": "Work through problems step-by-step and show your work",
                "science": "Connect theories to real-world examples and experiments", 
                "language": "Practice speaking and writing regularly",
                "history": "Create timelines and connect events to understand patterns"
            }
            
            for key, tip in subject_specific.items():
                if key in subject.lower():
                    tips.insert(0, tip)
                    break
        
        return tips[:3]
    
    async def _get_user_context(self, user_id: str, subject: str) -> Dict:
        """Get user's learning context from in-memory storage"""
        try:
            # Simple in-memory context - no database required
            return {
                "goals": ["understand_concepts", "improve_problem_solving"],
                "current_level": "intermediate",
                "recent_topics": [],
                "study_streak": 0
            }
            
        except Exception as e:
            logging.error(f"Error getting user context: {str(e)}")
            return {"goals": [], "current_level": "beginner", "recent_topics": []}
    
    async def _log_interaction(self, user_id: str, question: str, answer: str, subject: str):
        """Log interaction for analytics (in-memory only)"""
        try:
            # Simple logging - no database required
            logger.info(f"Interaction logged: user={user_id}, subject={subject}, model={self.model_name}")
        except Exception as e:
            logger.error(f"Error logging interaction: {str(e)}")
    
    async def generate_quiz_question(
        self, 
        subject: str, 
        topic: str, 
        difficulty: str = "intermediate",
        question_type: str = "multiple_choice"
    ) -> Dict[str, Any]:
        """Generate quiz questions for practice"""
        if not self.initialized:
            await self.initialize()
        
        prompt = f"""Create a {difficulty}-level {question_type} quiz question about {topic} in {subject}.

Format your response as:
Question: [Your question here]
Options: 
A) [Option A]
B) [Option B] 
C) [Option C]
D) [Option D]
Correct Answer: [Letter]
Explanation: [Why this is correct and others are wrong]
Study Tip: [One helpful tip for remembering this concept]"""

        try:
            response_text = await self._generate_with_llama(prompt)
            
            return {
                "quiz_content": response_text,
                "subject": subject,
                "topic": topic,
                "difficulty": difficulty,
                "question_type": question_type,
                "timestamp": datetime.utcnow().isoformat()
            }
        except Exception as e:
            logging.error(f"Quiz generation error: {str(e)}")
            raise
    
    async def summarize_content(
        self, 
        content: str, 
        summary_type: str = "key_points",
        max_length: int = 200
    ) -> Dict[str, Any]:
        """Summarize educational content"""
        if not self.initialized:
            await self.initialize()
        
        summary_prompts = {
            "key_points": f"Extract the key learning points from this content in bullet format:\n\n{content}\n\nKey Points:",
            "brief": f"Provide a brief summary of this educational content in {max_length} words or less:\n\n{content}\n\nSummary:",
            "detailed": f"Create a comprehensive summary covering all important concepts from:\n\n{content}\n\nDetailed Summary:"
        }
        
        prompt = summary_prompts.get(summary_type, summary_prompts["key_points"])
        
        try:
            summary_text = await self._generate_with_llama(prompt)
            
            return {
                "summary": summary_text,
                "original_length": len(content.split()),
                "summary_length": len(summary_text.split()),
                "compression_ratio": round(len(summary_text.split()) / len(content.split()), 2),
                "summary_type": summary_type,
                "timestamp": datetime.utcnow().isoformat()
            }
        except Exception as e:
            logging.error(f"Summarization error: {str(e)}")
            raise
    
    async def explain_concept(
        self, 
        concept: str, 
        subject: str, 
        difficulty: str = "intermediate",
        include_examples: bool = True
    ) -> Dict[str, Any]:
        """Provide detailed concept explanations"""
        if not self.initialized:
            await self.initialize()
        
        prompt = f"""Explain the concept of "{concept}" in {subject} for a {difficulty}-level student.

Your explanation should:
- Start with a clear, simple definition
- Break down complex parts step by step
- {"Include practical examples and analogies" if include_examples else "Focus on theoretical understanding"}
- Connect to related concepts
- End with why this concept is important

Concept: {concept}
Subject: {subject}
Level: {difficulty}

Explanation:"""

        try:
            explanation = await self._generate_with_llama(prompt)
            
            # Generate related concepts using embedding similarity
            related_concepts = await self._find_related_concepts(concept, subject)
            
            return {
                "concept": concept,
                "explanation": explanation,
                "subject": subject,
                "difficulty": difficulty,
                "related_concepts": related_concepts,
                "examples_included": include_examples,
                "timestamp": datetime.utcnow().isoformat()
            }
        except Exception as e:
            logging.error(f"Concept explanation error: {str(e)}")
            raise
    
    async def _find_related_concepts(self, concept: str, subject: str) -> List[str]:
        """Find related concepts using embeddings"""
        try:
            # Get concept embedding
            concept_embedding = self.embedding_model.encode([concept])
            
            # Predefined concept databases by subject
            concept_db = {
                "mathematics": ["algebra", "calculus", "geometry", "statistics", "trigonometry", "derivatives", "integrals", "equations", "functions", "probability"],
                "physics": ["mechanics", "thermodynamics", "electromagnetism", "quantum", "relativity", "waves", "energy", "momentum", "force", "acceleration"],
                "chemistry": ["atoms", "molecules", "reactions", "bonds", "periodic table", "acids", "bases", "organic", "inorganic", "kinetics"],
                "computer science": ["algorithms", "data structures", "programming", "databases", "networks", "security", "machine learning", "AI", "software engineering"],
                "biology": ["cells", "genetics", "evolution", "ecology", "anatomy", "physiology", "DNA", "proteins", "organisms", "ecosystems"]
            }
            
            subject_concepts = concept_db.get(subject.lower(), concept_db.get("mathematics", []))
            
            if len(subject_concepts) > 0:
                # Get embeddings for all concepts
                concept_embeddings = self.embedding_model.encode(subject_concepts)
                
                # Calculate similarities
                similarities = self.embedding_model.similarity(concept_embedding, concept_embeddings)[0]
                
                # Get top 3 most similar concepts
                top_indices = similarities.argsort()[-4:][::-1]  # Top 4 excluding self
                related = [subject_concepts[i] for i in top_indices if similarities[i] > 0.3]
                
                return related[:3]
            
            return ["Practice Problems", "Study Methods", "Additional Resources"]
            
        except Exception as e:
            logging.error(f"Related concepts error: {str(e)}")
            return ["Practice Problems", "Study Methods", "Additional Resources"]
    
    async def get_model_status(self) -> Dict[str, Any]:
        """Get current model status"""
        return {
            "model_name": self.model_name,
            "device": self.device,
            "initialized": self.initialized,
            "cuda_available": torch.cuda.is_available(),
            "memory_usage": self._get_memory_usage() if self.initialized else None
        }
    
    def _get_memory_usage(self) -> Dict[str, str]:
        """Get current memory usage"""
        try:
            if torch.cuda.is_available() and self.device == "cuda":
                return {
                    "gpu_memory_allocated": f"{torch.cuda.memory_allocated() / 1024**3:.2f} GB",
                    "gpu_memory_cached": f"{torch.cuda.memory_reserved() / 1024**3:.2f} GB"
                }
            return {"cpu_memory": "N/A for CPU inference"}
        except Exception:
            return {"memory_info": "Unable to retrieve"}

# Create singleton instance
llama_ai_service = LlamaAIService()
